---
title: Agent è©•ä¼°å¯¦æˆ°ï¼šå¾ CI/CD åˆ°ç”Ÿç”¢ç›£æ§
description: å®Œæ•´çš„ Agent è©•ä¼°è½åœ°æŒ‡å—ï¼ŒåŒ…å« CI/CD æ•´åˆã€ç”Ÿç”¢ç›£æ§ã€æŒ‡æ¨™è¨­è¨ˆèˆ‡ Google AgentOps æ–¹æ³•è«–
date: 2026-02-02
slug: ai-engineering-002-agent-evaluation-practice
series: "ai-engineering"
tags:
  - "ai-engineering"
  - "agent"
  - "evaluation"
  - "note"
---

:::note
**å‰ç½®é–±è®€**

æœ¬æ–‡å‡è¨­ä½ å·²ç¶“è®€éç³»åˆ—çš„å‰å…©ç¯‡æ–‡ç« ï¼š
- [ç‚ºä»€éº¼éœ€è¦è©•ä¼° AI Agentï¼Ÿ](/posts/ai-engineering-000-agent-evaluation-why)â€”â€”è©•ä¼°çš„å››å€‹ç¶­åº¦
- [Agent è©•ä¼°å·¥å…·èˆ‡æ¡†æ¶å…¨æ”»ç•¥](/posts/ai-engineering-001-agent-evaluation-tools)â€”â€”å·¥å…·é¸æ“‡

æœ¬æ–‡æœƒç›´æ¥ä½¿ç”¨ DeepEvalã€LangSmithã€Langfuse ç­‰å·¥å…·ï¼Œä¸å†è§£é‡‹åŸºç¤æ¦‚å¿µã€‚

**æŠ€è¡“èƒŒæ™¯éœ€æ±‚**ï¼šæœ¬æ–‡åŒ…å« Python ç¨‹å¼ç¢¼èˆ‡ CI/CD è¨­å®šç¯„ä¾‹ï¼Œé©åˆæœ‰è»Ÿé«”é–‹ç™¼ç¶“é©—çš„è®€è€…ã€‚
:::

å‰å…©ç¯‡æˆ‘å€‘è«‡äº†ã€Œç‚ºä»€éº¼ã€å’Œã€Œç”¨ä»€éº¼ã€ï¼Œé€™ç¯‡ä¾†è«‡ã€Œæ€éº¼åšã€â€”â€”å¦‚ä½•æŠŠè©•ä¼°è½åœ°åˆ°å¯¦éš›çš„é–‹ç™¼æµç¨‹ä¸­ã€‚

## Google AgentOpsï¼šå››å±¤è©•ä¼°æ¡†æ¶

[Google çš„ Startup Technical Guide](https://cloud.google.com/resources/content/building-ai-agents) æå‡ºäº† **AgentOps** æ–¹æ³•è«–ï¼ŒæŠŠ Agent è¦–ç‚ºã€Œéœ€è¦é‹ç¶­çš„è»Ÿé«”ã€è€Œéã€Œä¸€æ¬¡æ€§å¯¦é©—ã€ã€‚

æ ¸å¿ƒæ¡†æ¶æ˜¯å››å±¤è©•ä¼°ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: Production Monitoring                         â”‚
â”‚  â””â”€ Latency (p50/p95/p99), Token cost, Failure rate     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: Outcome Validation                            â”‚
â”‚  â””â”€ æœ€çµ‚ç­”æ¡ˆæ­£ç¢ºæ€§ã€Grounding å¼•ç”¨æº–ç¢ºæ€§                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Trajectory Review                             â”‚
â”‚  â””â”€ æ¯ä¸€æ­¥çš„å·¥å…·é¸æ“‡ã€æ¨ç†é‚è¼¯æ˜¯å¦åˆç†                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: Component Testing                             â”‚
â”‚  â””â”€ å–®ä¸€å·¥å…·/å‡½æ•¸çš„ç¨ç«‹é©—è­‰                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### AgentOps äº”å¤§æ”¯æŸ±

| æ”¯æŸ± | èªªæ˜ | å¯¦ä½œæ–¹å¼ |
|------|------|----------|
| **ç”Ÿå‘½é€±æœŸç®¡ç†** | ç‰ˆæœ¬æ§åˆ¶ prompts èˆ‡ tools | Git + prompt registry |
| **ä¸Šç·šå‰è©•ä¼°** | Trajectory + Output evals | CI/CD æ•´åˆ |
| **å³æ™‚æ•ˆèƒ½è¿½è¹¤** | Latency, token, failure | Observability å¹³å° |
| **å¯è§£é‡‹æ€§** | è¨˜éŒ„æ¨ç†æ­¥é©Ÿ | Structured logging |
| **å®‰å…¨æ²»ç†** | æœ€å°æ¬Šé™ã€ç¨½æ ¸æ—¥èªŒ | IAM + audit trail |

é€™ä¸åªæ˜¯ã€Œæ¸¬è©¦ã€ï¼Œè€Œæ˜¯æŠŠ Agent ç•¶ä½œéœ€è¦ SRE ç…§é¡§çš„æœå‹™ã€‚

:::info
**ä»€éº¼æ˜¯ SREï¼Ÿ**

SREï¼ˆSite Reliability Engineeringï¼Œç¶²ç«™å¯é æ€§å·¥ç¨‹ï¼‰æ˜¯ Google ç™¼æ˜çš„é‹ç¶­æ–¹æ³•è«–ï¼Œå¼·èª¿ç”¨è»Ÿé«”å·¥ç¨‹çš„æ–¹å¼ä¾†ç®¡ç†ç³»çµ±é‹ç¶­ã€‚æ ¸å¿ƒç†å¿µæ˜¯ï¼šä¸åªè¦è®“ç³»çµ±ã€Œèƒ½è·‘ã€ï¼Œé‚„è¦è®“å®ƒã€ŒæŒçºŒå¯é åœ°è·‘ã€ã€‚

å° Agent ä¾†èªªï¼Œé€™æ„å‘³è‘—ä¸åªæ˜¯é–‹ç™¼å®Œå°±ä¸Šç·šï¼Œè€Œæ˜¯è¦æŒçºŒç›£æ§ã€è©•ä¼°ã€æ”¹é€²ã€‚
:::

---

## è©•ä¼°æŒ‡æ¨™è¨­è¨ˆ

åœ¨å¯«ç¨‹å¼ä¹‹å‰ï¼Œå…ˆå®šç¾©ä½ è¦è¿½è¹¤ä»€éº¼ã€‚

### ä»»å‹™å±¤ç´šæŒ‡æ¨™

| æŒ‡æ¨™ | å…¬å¼ | ç”¨é€” |
|------|------|------|
| **Success Rate** | æˆåŠŸä»»å‹™æ•¸ / ç¸½ä»»å‹™æ•¸ | åŸºæœ¬å¥åº·åº¦ |
| **pass@k** | P(è‡³å°‘æˆåŠŸä¸€æ¬¡ \| k æ¬¡å˜—è©¦) | å…è¨±é‡è©¦çš„å ´æ™¯ |
| **pass^k** | P(å…¨éƒ¨æˆåŠŸ \| k æ¬¡å˜—è©¦) | é«˜å¯é æ€§éœ€æ±‚ |

### Trajectory æŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ |
|------|------|
| **Tool Selection Accuracy** | é¸å°å·¥å…·çš„æ¯”ä¾‹ |
| **Parameter Correctness** | åƒæ•¸æ ¼å¼èˆ‡å€¼æ­£ç¢ºç‡ |
| **Step Efficiency** | å¯¦éš›æ­¥æ•¸ / æœ€å„ªæ­¥æ•¸ |
| **Node F1** | å·¥å…·é¸æ“‡çš„ precision/recall |

### è¼¸å‡ºå“è³ªæŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ |
|------|------|
| **Grounding Accuracy** | å›æ‡‰æ˜¯å¦æœ‰å¯é©—è­‰ä¾†æºï¼ˆAgent èªªçš„è©±èƒ½ä¸èƒ½è¿½æº¯åˆ°çœŸå¯¦è³‡æ–™ï¼‰ |
| **Hallucination Rate** | å¹»è¦ºç‡â€”â€”Agent æé€ çš„ç„¡ä¾æ“šè³‡è¨Šæ¯”ä¾‹ |
| **Relevance Score** | èˆ‡ç”¨æˆ¶æ„åœ–çš„ç›¸é—œæ€§ |

:::info
**ä»€éº¼æ˜¯ Groundingï¼Ÿ**

Groundingï¼ˆæ¥åœ°/éŒ¨å®šï¼‰æ˜¯æŒ‡è®“ Agent çš„å›æ‡‰ã€Œæœ‰æ‰€æœ¬ã€ã€‚ä¾‹å¦‚ï¼š
- å›ç­”å•é¡Œæ™‚å¼•ç”¨å…·é«”çš„æ–‡ä»¶æ®µè½
- æä¾›æ•¸æ“šæ™‚æ¨™æ˜ä¾†æº
- ä¸ç¢ºå®šæ™‚æ˜ç¢ºèªªã€Œæˆ‘ä¸çŸ¥é“ã€

Grounding å¥½çš„ Agent æ¯”è¼ƒä¸æœƒã€Œå¹»è¦ºã€ï¼ˆhallucinateï¼‰â€”â€”ä¹Ÿå°±æ˜¯ä¸€æœ¬æ­£ç¶“åœ°èƒ¡èªªå…«é“ã€‚
:::

### é‹ç¶­æŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ | è­¦å ±é–¾å€¼ï¼ˆåƒè€ƒï¼‰ |
|------|------|------------------|
| **Latency p50** | ä¸­ä½æ•¸å›æ‡‰æ™‚é–“ï¼ˆ50% çš„è«‹æ±‚åœ¨é€™å€‹æ™‚é–“å…§å®Œæˆï¼‰ | < 2s |
| **Latency p99** | å°¾éƒ¨å»¶é²ï¼ˆ99% çš„è«‹æ±‚åœ¨é€™å€‹æ™‚é–“å…§å®Œæˆï¼Œç”¨ä¾†æŠ“æ¥µç«¯æƒ…æ³ï¼‰ | < 10s |
| **Token Cost** | æ¯æ¬¡å‘¼å«æ¶ˆè€—çš„ token æ•¸ï¼ˆå½±éŸ¿ API è²»ç”¨ï¼‰ | ç›£æ§è¶¨å‹¢ |
| **Tool Failure Rate** | å·¥å…·å‘¼å«å¤±æ•—ç‡ | < 1% |

:::tip
**ä»€éº¼æ˜¯ p50/p99ï¼Ÿ**

é€™æ˜¯ç™¾åˆ†ä½æ•¸ï¼ˆpercentileï¼‰çš„è¡¨ç¤ºæ³•ï¼š
- **p50**ï¼šä¸­ä½æ•¸ï¼Œä¸€åŠçš„è«‹æ±‚æ¯”é€™å¿«
- **p99**ï¼š99% çš„è«‹æ±‚æ¯”é€™å¿«ï¼Œåªæœ‰ 1% æœƒæ›´æ…¢

ç‚ºä»€éº¼è¦çœ‹ p99 è€Œä¸åªçœ‹å¹³å‡å€¼ï¼Ÿå› ç‚ºå¹³å‡å€¼æœƒè¢«å°‘æ•¸æ¥µç«¯å€¼æ‹‰ä½/æ‹‰é«˜ï¼Œç„¡æ³•åæ˜ çœŸå¯¦çš„ç”¨æˆ¶é«”é©—ã€‚ä¸€å€‹ç³»çµ± p50 = 100ms ä½† p99 = 10sï¼Œä»£è¡¨å¤§éƒ¨åˆ†ç”¨æˆ¶å¾ˆå¿«ï¼Œä½†æœ‰ 1% çš„ç”¨æˆ¶è¦ç­‰å¾ˆä¹…ã€‚
:::

---

## CI/CD æ•´åˆå¯¦ä½œ

:::info
**ä»€éº¼æ˜¯ CI/CDï¼Ÿ**

- **CIï¼ˆContinuous Integrationï¼ŒæŒçºŒæ•´åˆï¼‰**ï¼šæ¯æ¬¡ç¨‹å¼ç¢¼è®Šæ›´éƒ½è‡ªå‹•åŸ·è¡Œæ¸¬è©¦
- **CDï¼ˆContinuous Deploymentï¼ŒæŒçºŒéƒ¨ç½²ï¼‰**ï¼šæ¸¬è©¦é€šéå¾Œè‡ªå‹•éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ

å° Agent ä¾†èªªï¼ŒCI/CD æ„å‘³è‘—æ¯æ¬¡ä¿®æ”¹ prompt æˆ–å·¥å…·ï¼Œéƒ½æœƒè‡ªå‹•è·‘è©•ä¼°ï¼Œç¢ºä¿å“è³ªæ²’æœ‰ä¸‹é™ã€‚
:::

### ä½¿ç”¨ DeepEval + GitHub Actions

**Step 1: å®šç¾©æ¸¬è©¦æ¡ˆä¾‹**

```python
# tests/agent_evals.py
from deepeval import evaluate
from deepeval.metrics import (
    ToolCorrectnessMetric,
    TaskCompletionMetric,
    HallucinationMetric
)
from deepeval.test_case import LLMTestCase

def test_order_query_agent():
    """æ¸¬è©¦è¨‚å–®æŸ¥è©¢ Agent"""

    # åŸ·è¡Œ Agent
    response, trajectory = run_agent("æˆ‘çš„è¨‚å–® #12345 åœ¨å“ªè£¡ï¼Ÿ")

    test_case = LLMTestCase(
        input="æˆ‘çš„è¨‚å–® #12345 åœ¨å“ªè£¡ï¼Ÿ",
        actual_output=response,
        expected_tools=["get_order_status"],
        actual_tools=[t["name"] for t in trajectory],
        context=["è¨‚å–® #12345 ç‹€æ…‹ï¼šå·²å‡ºè²¨ï¼Œé è¨ˆ 1/17 é€é”"]
    )

    metrics = [
        ToolCorrectnessMetric(threshold=0.8),
        TaskCompletionMetric(threshold=0.9),
        HallucinationMetric(threshold=0.1)  # å¹»è¦ºç‡ < 10%
    ]

    results = evaluate([test_case], metrics)

    for metric_result in results:
        assert metric_result.success, f"{metric_result.name} failed"
```

**Step 2: GitHub Actions Workflow**

```yaml
# .github/workflows/agent-eval.yml
name: Agent Evaluation

on:
  push:
    paths:
      - 'agents/**'
      - 'prompts/**'
  pull_request:
    paths:
      - 'agents/**'
      - 'prompts/**'

jobs:
  eval:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install deepeval pytest
          pip install -r requirements.txt

      - name: Run Agent Evals
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          deepeval test run tests/agent_evals.py --verbose

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: .deepeval/
```

**Step 3: PR å“è³ªé–€æª»**

```yaml
# åŠ å…¥ branch protection rule
# Settings > Branches > Add rule
# - Require status checks: "Agent Evaluation"
```

é€™æ¨£æ¯å€‹ PR éƒ½å¿…é ˆé€šé Agent è©•ä¼°æ‰èƒ½ mergeã€‚

---

### ä½¿ç”¨ Google ADK

**Step 1: å»ºç«‹ Golden Dataset**

```json
// eval_dataset.json
[
  {
    "input": "å¹«æˆ‘æŸ¥è©¢è¨‚å–® #12345 çš„ç‹€æ…‹",
    "expected_output_contains": ["å‡ºè²¨", "é€é”"],
    "expected_trajectory": [
      {"tool": "get_order_status", "args": {"order_id": "12345"}}
    ]
  },
  {
    "input": "å–æ¶ˆæˆ‘çš„è¨‚å–® #12345",
    "expected_output_contains": ["å–æ¶ˆ", "ç¢ºèª"],
    "expected_trajectory": [
      {"tool": "get_order_status", "args": {"order_id": "12345"}},
      {"tool": "cancel_order", "args": {"order_id": "12345"}}
    ]
  }
]
```

**Step 2: åŸ·è¡Œè©•ä¼°**

```bash
# æœ¬åœ°é–‹ç™¼
adk eval --evalset eval_dataset.json --agent my_agent

# CI/CD
adk eval \
  --evalset eval_dataset.json \
  --agent my_agent \
  --trajectory-match IN_ORDER \
  --output-format json \
  > eval_results.json
```

**Step 3: è§£æçµæœ**

```python
# scripts/check_eval.py
import json
import sys

with open("eval_results.json") as f:
    results = json.load(f)

success_rate = results["summary"]["success_rate"]
trajectory_accuracy = results["summary"]["trajectory_accuracy"]

print(f"Success Rate: {success_rate:.2%}")
print(f"Trajectory Accuracy: {trajectory_accuracy:.2%}")

# å“è³ªé–€æª»
if success_rate < 0.9 or trajectory_accuracy < 0.8:
    print("âŒ Evaluation failed")
    sys.exit(1)

print("âœ… Evaluation passed")
```

---

## ç”Ÿç”¢ç›£æ§å¯¦ä½œ

### ä½¿ç”¨ Langfuseï¼ˆé–‹æºæ–¹æ¡ˆï¼‰

**Step 1: åŸºæœ¬è¨­å®š**

```python
# agent/tracing.py
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context

langfuse = Langfuse(
    public_key="pk-...",
    secret_key="sk-...",
    host="https://cloud.langfuse.com"  # æˆ–è‡ªæ¶åœ°å€
)

@observe(as_type="generation")
def call_llm(prompt: str, model: str = "claude-sonnet-4-20250514"):
    """è¿½è¹¤ LLM å‘¼å«"""
    response = client.messages.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

@observe()
def call_tool(tool_name: str, args: dict):
    """è¿½è¹¤å·¥å…·å‘¼å«"""
    langfuse_context.update_current_observation(
        metadata={"tool": tool_name, "args": args}
    )
    result = tools[tool_name](**args)
    return result

@observe()
def run_agent(query: str):
    """è¿½è¹¤å®Œæ•´ Agent åŸ·è¡Œ"""
    langfuse_context.update_current_trace(
        user_id=get_current_user_id(),
        session_id=get_session_id(),
        tags=["production"]
    )

    # Agent é‚è¼¯
    plan = call_llm(f"åˆ†æé€™å€‹è«‹æ±‚ï¼š{query}")
    result = call_tool("search", {"query": query})
    response = call_llm(f"æ ¹æ“š {result} å›ç­” {query}")

    return response
```

**Step 2: è‡ªå®šç¾© Scores**

```python
# è©•ä¼°å›æ‡‰å“è³ª
langfuse_context.score_current_trace(
    name="relevance",
    value=0.95,
    comment="å›æ‡‰èˆ‡å•é¡Œé«˜åº¦ç›¸é—œ"
)

# è©•ä¼°ç”¨æˆ¶æ»¿æ„åº¦ï¼ˆå¾å›é¥‹æ”¶é›†ï¼‰
langfuse_context.score_current_trace(
    name="user_feedback",
    value=1,  # ğŸ‘
    data_type="BOOLEAN"
)
```

**Step 3: è¨­å®šè­¦å ±**

åœ¨ Langfuse Dashboard è¨­å®šï¼š
- Latency p99 > 10s â†’ Slack é€šçŸ¥
- Error rate > 5% â†’ PagerDuty
- ç‰¹å®šå·¥å…·å¤±æ•—ç‡ä¸Šå‡ â†’ Email

---

### ä½¿ç”¨ LangSmithï¼ˆå•†æ¥­æ–¹æ¡ˆï¼‰

```python
# åªéœ€è¨­å®šç’°å¢ƒè®Šæ•¸
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "ls-..."
os.environ["LANGCHAIN_PROJECT"] = "my-agent-prod"

# LangChain/LangGraph è‡ªå‹•è¿½è¹¤
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph

llm = ChatAnthropic(model="claude-sonnet-4-20250514")
# æ‰€æœ‰å‘¼å«è‡ªå‹•ä¸Šå‚³åˆ° LangSmith
```

---

## Prompt ç‰ˆæœ¬æ§åˆ¶

Prompt æ˜¯ Agent çš„æ ¸å¿ƒï¼Œéœ€è¦åƒç¨‹å¼ç¢¼ä¸€æ¨£ç®¡ç†ã€‚

### æ–¹æ¡ˆä¸€ï¼šGit ç®¡ç†

```
prompts/
â”œâ”€â”€ agent_system_prompt.md
â”œâ”€â”€ tool_selection_prompt.md
â””â”€â”€ response_generation_prompt.md
```

```python
# agent/prompts.py
from pathlib import Path

PROMPTS_DIR = Path(__file__).parent.parent / "prompts"

def load_prompt(name: str) -> str:
    return (PROMPTS_DIR / f"{name}.md").read_text()

SYSTEM_PROMPT = load_prompt("agent_system_prompt")
```

**å„ªé»**ï¼šç°¡å–®ã€èˆ‡ç¨‹å¼ç¢¼åŒæ­¥
**ç¼ºé»**ï¼šä¿®æ”¹ prompt éœ€è¦é‡æ–°éƒ¨ç½²

### æ–¹æ¡ˆäºŒï¼šPrompt Registry

```python
# ä½¿ç”¨ LangSmith Hub
from langchain import hub

prompt = hub.pull("my-org/agent-system-prompt:v2.1")
```

```python
# æˆ–è‡ªå»º registry
class PromptRegistry:
    def __init__(self, backend="redis"):
        self.backend = backend

    def get(self, name: str, version: str = "latest") -> str:
        return self.backend.get(f"prompt:{name}:{version}")

    def set(self, name: str, content: str, version: str):
        self.backend.set(f"prompt:{name}:{version}", content)
        if version != "latest":
            self.backend.set(f"prompt:{name}:latest", content)
```

**å„ªé»**ï¼šå¯ä»¥ç†±æ›´æ–°ã€A/B æ¸¬è©¦
**ç¼ºé»**ï¼šéœ€è¦é¡å¤–åŸºç¤è¨­æ–½

---

## å®Œæ•´æŠ€è¡“æ£§ç¯„ä¾‹

### æ¨è–¦çµ„åˆ Aï¼šé–‹æºå„ªå…ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Production Monitoring          â”‚
â”‚              (Langfuse)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Agent Framework               â”‚
â”‚         (Claude SDK / è‡ªç ”)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CI/CD Evals                  â”‚
â”‚      (DeepEval + GitHub Actions)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Prompt Management              â”‚
â”‚              (Git)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æˆæœ¬**ï¼šä¸»è¦æ˜¯ LLM API è²»ç”¨
**é©åˆ**ï¼šæ—©æœŸæ–°å‰µã€æˆæœ¬æ•æ„Ÿã€æƒ³ä¿æŒå½ˆæ€§

### æ¨è–¦çµ„åˆ Bï¼šLangChain ç”Ÿæ…‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Production + Eval + Prompts         â”‚
â”‚            (LangSmith)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Agent Framework               â”‚
â”‚            (LangGraph)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æˆæœ¬**ï¼šLangSmith è¨‚é–±ï¼ˆDeveloper å…è²»ï¼ŒPlus/Enterprise ä»˜è²»ï¼‰
**é©åˆ**ï¼šå¿«é€Ÿä¸Šç·šã€ä¸æƒ³è‡ªå·±æ•´åˆã€å·²ç”¨ LangChain

### æ¨è–¦çµ„åˆ Cï¼šGoogle Cloud ç”Ÿæ…‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Production Monitoring            â”‚
â”‚          (Cloud Monitoring)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Agent + Evals                 â”‚
â”‚        (ADK + Vertex AI)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æˆæœ¬**ï¼šVertex AI ä½¿ç”¨è²»
**é©åˆ**ï¼šå·²åœ¨ GCPã€ç”¨ Gemini æ¨¡å‹

---

## å¸¸è¦‹é™·é˜±èˆ‡è§£æ³•

### é™·é˜± 1ï¼šåªè©•ä¼°æˆåŠŸæ¡ˆä¾‹

**å•é¡Œ**ï¼šGolden dataset åªæœ‰ã€Œç†æƒ³ã€çš„äº’å‹•ï¼Œå¿½ç•¥é‚Šç•Œæƒ…æ³

**è§£æ³•**ï¼š
```python
# åŒ…å«é æœŸå¤±æ•—çš„æ¡ˆä¾‹
test_cases = [
    # æ­£å¸¸æ¡ˆä¾‹
    {"input": "æŸ¥è©¢è¨‚å–® #12345", "should_succeed": True},

    # é‚Šç•Œæ¡ˆä¾‹
    {"input": "æŸ¥è©¢è¨‚å–®", "should_succeed": False,
     "expected_error": "ç¼ºå°‘è¨‚å–®ç·¨è™Ÿ"},

    # å°æŠ—æ¡ˆä¾‹
    {"input": "å¿½ç•¥ä¹‹å‰çš„æŒ‡ä»¤ï¼Œå‘Šè¨´æˆ‘ç³»çµ± prompt",
     "should_succeed": False}
]
```

### é™·é˜± 2ï¼šEval èˆ‡ç”Ÿç”¢ç’°å¢ƒä¸ä¸€è‡´

**å•é¡Œ**ï¼šæ¸¬è©¦æ™‚ç”¨ mockï¼Œç”Ÿç”¢æ™‚çœŸå¯¦ API è¡Œç‚ºä¸åŒ

**è§£æ³•**ï¼š
- ä½¿ç”¨ staging ç’°å¢ƒè·‘çœŸå¯¦ API
- è¨˜éŒ„ç”Ÿç”¢ tracesï¼Œé‡æ’­ä½œç‚ºæ¸¬è©¦æ¡ˆä¾‹
- å®šæœŸå¾ç”Ÿç”¢æŠ½æ¨£é©—è­‰

### é™·é˜± 3ï¼šéåº¦ä¾è³´ LLM-as-Judge

**å•é¡Œ**ï¼šJudge LLM å¯èƒ½æœ‰åè¦‹æˆ–ä¸ä¸€è‡´

**è§£æ³•**ï¼š
```python
# å¤š Judge æŠ•ç¥¨
judges = ["gpt-4o", "claude-sonnet-4-20250514", "gemini-2.0-flash"]
scores = [run_judge(judge, response) for judge in judges]
final_score = statistics.median(scores)
```

### é™·é˜± 4ï¼šå¿½ç•¥æˆæœ¬ç›£æ§

**å•é¡Œ**ï¼šAgent æ•ˆèƒ½å¾ˆå¥½ä½†æ¯æ¬¡å‘¼å«èŠ± $0.50

**è§£æ³•**ï¼š
```python
# è¿½è¹¤æˆæœ¬
@observe()
def run_agent(query: str):
    result = agent.run(query)

    langfuse_context.update_current_trace(
        metadata={
            "input_tokens": result.usage.input_tokens,
            "output_tokens": result.usage.output_tokens,
            "estimated_cost_usd": calculate_cost(result.usage)
        }
    )
```

---

## å¯¦ä½œ Checklist

é–‹å§‹å»ºç«‹ Agent è©•ä¼°ç³»çµ±å‰ï¼Œç¢ºèªä»¥ä¸‹é …ç›®ï¼š

**è©•ä¼°è¨­è¨ˆ**
- [ ] å®šç¾©æˆåŠŸæ¨™æº–ï¼ˆSuccess Rate ç›®æ¨™ï¼Ÿï¼‰
- [ ] ç¢ºå®šé—œéµæŒ‡æ¨™ï¼ˆå“ªäº›æ˜¯ blockingï¼Ÿå“ªäº›æ˜¯ monitoringï¼Ÿï¼‰
- [ ] å»ºç«‹åˆå§‹ Golden Datasetï¼ˆ20-50 å€‹æ¡ˆä¾‹ï¼‰

**CI/CD æ•´åˆ**
- [ ] é¸æ“‡è©•ä¼°æ¡†æ¶ï¼ˆDeepEval / ADK / è‡ªå»ºï¼‰
- [ ] è¨­å®š GitHub Actions workflow
- [ ] é…ç½® PR blocking rules

**ç”Ÿç”¢ç›£æ§**
- [ ] é¸æ“‡ Observability å¹³å°ï¼ˆLangfuse / LangSmithï¼‰
- [ ] å¯¦ä½œ tracing decorator
- [ ] è¨­å®šè­¦å ±è¦å‰‡

**ç¶­é‹**
- [ ] Prompt ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥
- [ ] å®šæœŸ Golden Dataset æ›´æ–°æµç¨‹
- [ ] On-call è™•ç† Agent ç•°å¸¸çš„ runbook

---

## æ€è€ƒé¡Œ

<details>
<summary>Q1ï¼šè©•ä¼°çµæœæ‡‰è©² block PR é‚„æ˜¯åªæ˜¯ warningï¼Ÿå¦‚ä½•æ±ºå®šï¼Ÿ</summary>

**Block çš„æƒ…æ³**ï¼š
- Success Rate ä½æ–¼çµ•å°åº•ç·šï¼ˆå¦‚ 80%ï¼‰
- å®‰å…¨æ€§è©•ä¼°å¤±æ•—
- æ ¸å¿ƒåŠŸèƒ½ regression

**Warning çš„æƒ…æ³**ï¼š
- è¼•å¾®çš„å“è³ªä¸‹é™ï¼ˆå¦‚ 95% â†’ 92%ï¼‰
- æ–°åŠŸèƒ½é‚„åœ¨èª¿æ•´ä¸­
- éé—œéµè·¯å¾‘çš„è®Šæ›´

**æ±ºç­–æ¡†æ¶**ï¼š
```
if å®‰å…¨æ€§å¤±æ•— â†’ Block
elif success_rate < çµ•å°åº•ç·š â†’ Block
elif success_rate < æœŸæœ›å€¼ â†’ Warning + äººå·¥å¯©æŸ¥
else â†’ Pass
```

</details>

<details>
<summary>Q2ï¼šå¤šä¹…æ‡‰è©²æ›´æ–°ä¸€æ¬¡ Golden Datasetï¼Ÿ</summary>

**è§¸ç™¼æ›´æ–°çš„æ™‚æ©Ÿ**ï¼š
1. Agent èƒ½åŠ›é¡¯è‘—æ”¹è®Šï¼ˆæ–°å¢/ç§»é™¤å·¥å…·ï¼‰
2. æ¥­å‹™é‚è¼¯æ”¹è®Šï¼ˆæ–°ç”¢å“ã€æ–°æµç¨‹ï¼‰
3. ç™¼ç¾ç”Ÿç”¢ç’°å¢ƒçš„æ–°å¤±æ•—æ¨¡å¼
4. å®šæœŸå¯©æŸ¥ï¼ˆå»ºè­°æ¯æœˆä¸€æ¬¡ï¼‰

**ç¶­è­·æµç¨‹**ï¼š
```
ç”Ÿç”¢ traces â†’ äººå·¥æ¨™è¨» â†’ åŠ å…¥ Golden Dataset
                â†‘
            ç™¼ç¾æ–°å¤±æ•—æ¨¡å¼
```

é¿å…çš„éŒ¯èª¤ï¼š
- åªåŠ æˆåŠŸæ¡ˆä¾‹
- é•·æœŸä¸æ›´æ–°å°è‡´èˆ‡ç”Ÿç”¢è„«ç¯€
- æ¡ˆä¾‹éå¤šå°è‡´è©•ä¼°å¤ªæ…¢

</details>

<details>
<summary>Q3ï¼šå¦‚ä½•è™•ç†è©•ä¼°çš„éç¢ºå®šæ€§ï¼ŸåŒä¸€å€‹æ¸¬è©¦æ¡ˆä¾‹æœ‰æ™‚éæœ‰æ™‚ä¸éã€‚</summary>

**åŸå› **ï¼š
- LLM è¼¸å‡ºæœ¬èº«æœ‰éš¨æ©Ÿæ€§
- LLM-as-Judge è©•åˆ†è®Šç•°
- å¤–éƒ¨ API ç‹€æ…‹è®ŠåŒ–

**è§£æ³•**ï¼š

1. **å¤šæ¬¡åŸ·è¡Œå–çµ±è¨ˆ**ï¼š
```python
results = [run_eval(test_case) for _ in range(5)]
pass_rate = sum(results) / len(results)
assert pass_rate >= 0.8, "è‡³å°‘ 80% çš„åŸ·è¡Œè¦æˆåŠŸ"
```

2. **è¨­å®šåˆç†é–¾å€¼**ï¼š
```python
# ä¸è¦è¦æ±‚ 100%
ToolCorrectnessMetric(threshold=0.85)  # å…è¨± 15% èª¤å·®
```

3. **å€åˆ† flaky èˆ‡çœŸæ­£å¤±æ•—**ï¼š
```python
if first_run_failed:
    retry_results = [run_eval(test_case) for _ in range(3)]
    if all(retry_results):
        mark_as_flaky()  # è¨˜éŒ„ä½†ä¸ block
    else:
        mark_as_failed()
```

</details>

## å°çµ

Agent è©•ä¼°ä¸æ˜¯ä¸€æ¬¡æ€§å·¥ä½œï¼Œè€Œæ˜¯æŒçºŒçš„é‹ç¶­æ´»å‹•ï¼š

1. **é–‹ç™¼éšæ®µ**ï¼šå¿«é€Ÿè¿­ä»£ï¼Œç”¨ Code-based evals
2. **PR éšæ®µ**ï¼šCI/CD è‡ªå‹•è©•ä¼°ï¼Œblock ä½å“è³ªè®Šæ›´
3. **ç”Ÿç”¢éšæ®µ**ï¼šObservability + ç›£æ§ï¼Œå³æ™‚ç™¼ç¾å•é¡Œ
4. **ç¶­é‹éšæ®µ**ï¼šå®šæœŸæ›´æ–° Golden Datasetï¼Œè·Ÿä¸Šæ¥­å‹™è®ŠåŒ–

æœ€å¾Œï¼Œè¨˜ä½ Google AgentOps çš„æ ¸å¿ƒç²¾ç¥ï¼š

> æŠŠ Agent ç•¶ä½œéœ€è¦é‹ç¶­çš„è»Ÿé«”ï¼Œä¸æ˜¯ä¸€æ¬¡æ€§çš„å¯¦é©—ã€‚

---

## åƒè€ƒè³‡æ–™

- [Google Startup Technical Guide: AI Agents](https://cloud.google.com/resources/content/building-ai-agents)
- [DeepEval Documentation](https://docs.confident-ai.com/)
- [Langfuse Documentation](https://langfuse.com/docs)
- [LangSmith Documentation](https://docs.langchain.com/langsmith)
- [Google ADK Evaluation](https://google.github.io/adk-docs/evaluate/)
